{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /home/sunzid/anaconda3/lib/python3.12/site-packages (3.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/sunzid/anaconda3/lib/python3.12/site-packages (from h5py) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['geometry', 'inputs', 'outputs']\n",
      "first_geometry shape:  (3,) first_geometry:  [0.08914796 0.16880947 0.10318343] \n",
      "\n",
      "first_input shape:  (11,) first_input:  [0.08572294 0.61110255 0.85597203 0.82917943 0.00633835 0.73598566\n",
      " 0.30098127 0.25647371 0.17699116 0.09897413 0.00674982] \n",
      "\n",
      "first_output shape:  (22,) first_output:  [0.02365114 0.15301892 0.12525921 0.09508325 0.06724529 0.04895108\n",
      " 0.02794279 0.02531836 0.01383378 0.0101849  0.00736416 0.56117202\n",
      " 0.64358048 0.63291136 0.6215687  0.60499423 0.60844916 0.58988128\n",
      " 0.5607913  0.51324512 0.48958556 0.47816882] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keys: ['geometry', 'inputs', 'outputs']\n",
      "first_geometry shape:  (3,) first_geometry:  [0.43138777 0.04664672 0.18537623] \n",
      "\n",
      "first_input shape:  (14,) first_input:  [0.3338711  0.58466833 0.69590435 0.95397432 0.52926811 0.89549399\n",
      " 0.72597417 0.74137618 0.19366376 0.40013094 0.98825008 0.07634631\n",
      " 0.9805122  0.84911726] \n",
      "\n",
      "first_output shape:  (22,) first_output:  [0.03969655 0.09654982 0.08385258 0.0716215  0.05741165 0.05011574\n",
      " 0.04133835 0.03916471 0.03380633 0.03102674 0.02968379 0.48890381\n",
      " 0.62460913 0.61192344 0.59475986 0.56691323 0.5542926  0.51194333\n",
      " 0.49033868 0.45832174 0.45026065 0.45405636] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import glob\n",
    "\n",
    "path = '/home/sunzid/Research/23-24_Automatic-Control-Lab/Projects/24_Aerosol/data/'\n",
    "pace318_train = glob.glob(path + 'pace318_train_set/H5/*.h5')\n",
    "\n",
    "rowNum = 3\n",
    "\n",
    "if pace318_train:\n",
    "    with h5py.File(pace318_train[0], 'r') as file:\n",
    "        print(\"Keys: %s\" % list(file.keys()))\n",
    "        first_geometry = file['geometry'][rowNum]\n",
    "        first_input = file['inputs'][rowNum]\n",
    "        first_output = file['outputs'][rowNum]\n",
    "\n",
    "        print(\"first_geometry shape: \", first_geometry.shape, \n",
    "              \"first_geometry: \", first_geometry, '\\n')\n",
    "        print(\"first_input shape: \", first_input.shape,\n",
    "              \"first_input: \", first_input, '\\n')\n",
    "        print(\"first_output shape: \", first_output.shape,\n",
    "              \"first_output: \", first_output, '\\n')\n",
    "        print('\\n\\n')\n",
    "\n",
    "else:\n",
    "    print(\"File not found\")\n",
    "\n",
    "pace325_train = glob.glob(path + 'pace325_train_set/H5/*.h5')\n",
    "\n",
    "if pace325_train:\n",
    "    with h5py.File(pace325_train[0], 'r') as file:\n",
    "        print(\"Keys: %s\" % list(file.keys()))\n",
    "        first_geometry = file['geometry'][rowNum]\n",
    "        first_input = file['inputs'][rowNum]\n",
    "        first_output = file['outputs'][rowNum]\n",
    "\n",
    "        print(\"first_geometry shape: \", first_geometry.shape, \n",
    "              \"first_geometry: \", first_geometry, '\\n')\n",
    "        print(\"first_input shape: \", first_input.shape,\n",
    "              \"first_input: \", first_input, '\\n')\n",
    "        print(\"first_output shape: \", first_output.shape,\n",
    "              \"first_output: \", first_output, '\\n')\n",
    "else:\n",
    "    print(\"File not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Inputs: torch.Size([32, 11])\n",
      "Outputs: torch.Size([32, 22])\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "import h5py\n",
    "import glob\n",
    "\n",
    "# Custom Dataset class\n",
    "class H5Dataset(Dataset):\n",
    "    def __init__(self, file_paths):\n",
    "        self.file_paths = file_paths\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Assuming all files have the same number of rows for simplicity\n",
    "        with h5py.File(self.file_paths[0], 'r') as file:\n",
    "            return file['geometry'].shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load data from the first file for simplicity (can extend this for multiple files)\n",
    "        with h5py.File(self.file_paths[0], 'r') as file:\n",
    "            input_data = file['inputs'][idx]\n",
    "            output_data = file['outputs'][idx]\n",
    "        \n",
    "        # Convert numpy arrays to PyTorch tensors\n",
    "        input_tensor = torch.tensor(input_data, dtype=torch.float32)\n",
    "        output_tensor = torch.tensor(output_data, dtype=torch.float32)\n",
    "        \n",
    "        return input_tensor, output_tensor\n",
    "\n",
    "# Path to the .h5 files\n",
    "path = '/home/sunzid/Research/23-24_Automatic-Control-Lab/Projects/24_Aerosol/data/'\n",
    "pace318_train = glob.glob(path + 'pace318_train_set/H5/chunk1.h5')\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = H5Dataset(pace318_train)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Example: Iterate over the dataloader\n",
    "for batch_idx, (inputs, outputs) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx + 1}\")\n",
    "    print(f\"Inputs: {inputs.shape}\")\n",
    "    print(f\"Outputs: {outputs.shape}\")\n",
    "    break  # Just showing one batch as an example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron for regression.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(14, 64),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(64, 32),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(32, 22)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''\n",
    "      Forward pass\n",
    "    '''\n",
    "    return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MLP\n",
    "mlp = MLP()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 0.0211\n",
      "Epoch [2/15], Loss: 0.0203\n",
      "Epoch [3/15], Loss: 0.0194\n",
      "Epoch [4/15], Loss: 0.0183\n",
      "Epoch [5/15], Loss: 0.0172\n",
      "Epoch [6/15], Loss: 0.0163\n",
      "Epoch [7/15], Loss: 0.0156\n",
      "Epoch [8/15], Loss: 0.0150\n",
      "Epoch [9/15], Loss: 0.0146\n",
      "Epoch [10/15], Loss: 0.0142\n",
      "Epoch [11/15], Loss: 0.0139\n",
      "Epoch [12/15], Loss: 0.0136\n",
      "Epoch [13/15], Loss: 0.0134\n",
      "Epoch [14/15], Loss: 0.0132\n",
      "Epoch [15/15], Loss: 0.0131\n",
      "Training complete and model saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Check if a GPU is available, otherwise fall back to CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the regression model (a simple feedforward network)\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model and move it to the GPU (if available)\n",
    "input_size = 11  # Input shape (11 features)\n",
    "output_size = 22  # Output shape (22 features)\n",
    "model = RegressionModel(input_size, output_size).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
    "\n",
    "# Create the dataset and dataloader (as in the previous code)\n",
    "dataset = H5Dataset(pace318_train)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 15  # Number of training epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0  # Track the total loss for this epoch\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        # Move inputs and targets to the GPU (if available)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: compute the model output\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass: compute the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'regression_model.pth')\n",
    "\n",
    "print(\"Training complete and model saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
