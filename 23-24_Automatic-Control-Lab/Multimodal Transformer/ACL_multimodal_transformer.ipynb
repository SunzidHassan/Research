{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Navigation table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Paper**    | **Year** | **Design Structure** | **Training Method** | **Dataset**              | **Application**                     | **Source Code**                             | **Inference Time/ Hardware** |\n",
    "|--------------|----------|----------------------|---------------------|--------------------------|-------------------------------------|---------------------------------------------|------------------------------|\n",
    "| **CoW**      | 2022     | CLIP                 |                     | RoboTHOR                 | Inddor scenes                       | https://github.com/real-stanford/cow.git    |                              |\n",
    "| **ZSON**     | 2022     | CLIP                 |                     | MP3D                     | Open-world object-goal navigation   | https://github.com/gunagg/zson.git          |                              |\n",
    "| **LM-Nav**   | 2022     | ViNG, CLIP, GPT-3    |                     |                          | Outdoor environments                | https://github.com/blazejosinski/lm_nav.git |                              |\n",
    "| **CLIP-NAV** | 2022     | CLIP                 |                     | REVERIE                  | Household environments              |                                             |                              |\n",
    "| **SQA3D**    | 2023     | CLIP, BERT, GPT-3    |                     | ScanNet                  | Situated question answering         | https://github.com/SilongYong/SQA3D.git     |                              |\n",
    "| **L3MVN**    | 2023     | RoBERTa              |                     | Gibson, HM3D             | Indoor scenes                       | https://github.com/ybgdgh/L3MVN.git         |                              |\n",
    "| **VLMAP**    | 2023     | CLIP                 |                     | Matterport 3D, Habitat   | Navigation, obstacle map generation | https://github.com/vlmaps/vlmaps.git        |                              |\n",
    "| **OVRL**     | 2023     | ViT, LSTM            |                     | HM3D, Gibson             | IMAGENAV, OBJECTNAV                 | https://github.com/ykarmesh/OVRL.git        |                              |\n",
    "| **ESC**      | 2023     | PSL, GLIP            |                     | HM3D, RoboTHOR           | Indoor scenes                       |                                             |                              |\n",
    "| **NavGPT**   | 2023     | GPT-4                |                     | R2R, R2R-VLN             | Indoor scenes                       | https://github.com/GengzeZhou/NavGPT.git    |                              |\n",
    "| **VELMA**    | 2023     | CLIP                 |                     | Touchdown, Map2seq       | Urban VLN                           | https://github.com/raphael-sch/VELMA.git    |                              |\n",
    "| **$A^2$Nav** | 2023     | GPT-3                |                     | R2R-Habitat, RxR-Habitat | Zero-shot navigation                |                                             |                              |\n",
    "| **MiC**      | 2023     | CLIP                 |                     | REVERIE                  | Remote object localization          | https://github.com/YanyuanQiao/MiC.git      |                              |\n",
    "| **SayNav**   | 2023     | GPT                  |                     | Habitat, Gibson-4+       | Multi-object navigation             |                                             |                              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoW\n",
    "\n",
    "\n",
    "### ZSON\n",
    "\n",
    "\n",
    "### LM-Nav (https://arxiv.org/pdf/2207.04429.pdf)\n",
    "- GPT-3 as LLM for decoding verbal instructions into a series of textual landmarks.\n",
    "- CLIP as VLM for anchoring textual landmarks to a topological map.\n",
    "- Vision-Action Model is a self-supervised robotic control model for utilizing visual data and executing physical actions based on plans synthesized by LLM and VLM.\n",
    "\n",
    "\n",
    "### CLIP-Nav\n",
    "\n",
    "\n",
    "### SQA3D\n",
    "\n",
    "\n",
    "### L3MVN\n",
    "\n",
    "\n",
    "### VLMAP\n",
    "\n",
    "\n",
    "### OVRL\n",
    "\n",
    "\n",
    "### ESC\n",
    "\n",
    "\n",
    "### VELMA\n",
    "\n",
    "### $A^2$NAV\n",
    "\n",
    "\n",
    "### MiC\n",
    "\n",
    "\n",
    "### SayNav\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
