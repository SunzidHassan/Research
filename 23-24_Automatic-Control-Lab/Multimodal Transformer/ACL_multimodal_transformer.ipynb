{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Navigation table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Paper**    | **Paper Link**                   | **Year** | **Design Structure** | **Training Method** | **Dataset**              | **Application**                     | **Source Code**                             | **Inference Time/ Hardware** |\n",
    "|--------------|----------------------------------|----------|----------------------|---------------------|--------------------------|-------------------------------------|---------------------------------------------|------------------------------|\n",
    "| **CoW**      | https://arxiv.org/abs/2203.10421 | 2022     | CLIP                 | Zero-shot           | RoboTHOR                 | Inddor scenes                       | https://github.com/real-stanford/cow.git    |                              |\n",
    "| **ZSON**     | https://arxiv.org/abs/2206.12403 | 2022     | CLIP                 | Fine-tuning         | MP3D                     | Open-world object-goal navigation   | https://github.com/gunagg/zson.git          |                              |\n",
    "| **LM-Nav**   | https://arxiv.org/abs/2207.04429 | 2022     | ViNG, CLIP, GPT-3    | Zero-shot           |                          | Outdoor environments                | https://github.com/blazejosinski/lm_nav.git |                              |\n",
    "| **CLIP-NAV** | https://arxiv.org/abs/2211.16649 | 2022     | CLIP                 | Zero-shot           | REVERIE                  | Household environments              |                                             |                              |\n",
    "| **SQA3D**    | https://arxiv.org/abs/2210.07474 | 2023     | CLIP, BERT, GPT-3    | Fine-tuning         | ScanNet                  | Situated question answering         | https://github.com/SilongYong/SQA3D.git     |                              |\n",
    "| **L3MVN**    | https://arxiv.org/abs/2304.05501 | 2023     | RoBERTa              | Zero-shot           | Gibson, HM3D             | Indoor scenes                       | https://github.com/ybgdgh/L3MVN.git         |                              |\n",
    "| **VLMAP**    | https://arxiv.org/abs/2210.05714 | 2023     | CLIP                 | Zero-shot           | Matterport 3D, Habitat   | Navigation, obstacle map generation | https://github.com/vlmaps/vlmaps.git        |                              |\n",
    "| **OVRL**     | https://arxiv.org/abs/2204.13226 | 2023     | ViT, LSTM            | New architecture         | HM3D, Gibson             | IMAGENAV, OBJECTNAV                 | https://github.com/ykarmesh/OVRL.git        |                              |\n",
    "| **ESC**      | https://arxiv.org/abs/2301.13166 | 2023     | PSL, GLIP            | Zero-shot           | HM3D, RoboTHOR           | Indoor scenes                       |                                             |                              |\n",
    "| **NavGPT**   | https://arxiv.org/abs/2305.16986 | 2023     | GPT-4                | Zero-shot           | R2R, R2R-VLN             | Indoor scenes                       | https://github.com/GengzeZhou/NavGPT.git    |                              |\n",
    "| **VELMA**    | https://arxiv.org/abs/2307.06082 | 2023     | CLIP                 | Fine-tuning         | Touchdown, Map2seq       | Urban VLN                           | https://github.com/raphael-sch/VELMA.git    |                              |\n",
    "| **$A^2$Nav** | https://arxiv.org/abs/2308.07997 | 2023     | GPT-3                | Zero-shot           | R2R-Habitat, RxR-Habitat | Zero-shot navigation                |                                             |                              |\n",
    "| **MiC**      | https://arxiv.org/abs/2308.10141 | 2023     | CLIP                 | Zero-shot           | REVERIE                  | Remote object localization          | https://github.com/YanyuanQiao/MiC.git      |                              |\n",
    "| **SayNav**   | https://arxiv.org/abs/2309.04077 | 2023     | GPT                  |                     | Habitat, Gibson-4+       | Multi-object navigation             |                                             |                              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoW\n",
    "Decomposition of object navigation task into zero-shot object localization and exploration, where CLIP model is used as the object localizer, and there are two exploration methods - learnable and traditional.\n",
    "CoW commences exploration when the object remains undetected and advances towards the target upon its identification. CoW uses three techniques for localization: gradient-based, k-patch-based, k-language-based.\n",
    "\n",
    "\n",
    "### ZSON\n",
    "\n",
    "\n",
    "### LM-Nav (https://arxiv.org/pdf/2207.04429.pdf)\n",
    "- GPT-3 as LLM for decoding verbal instructions into a series of textual landmarks.\n",
    "- CLIP as VLM for anchoring textual landmarks to a topological map.\n",
    "- Vision-Action Model is a self-supervised robotic control model for utilizing visual data and executing physical actions based on plans synthesized by LLM and VLM.\n",
    "\n",
    "\n",
    "### CLIP-Nav\n",
    "Dissect guidance language into critical keyphrases, visually anchor them with CLIP, use the grounding scores and the current state to predict navigation action.\n",
    "\n",
    "\n",
    "### SQA3D\n",
    "\n",
    "\n",
    "### L3MVN\n",
    "2 module: language module handles natural language instructions, generating a semantic map embedded with general physical world knowledge. The later employs this semantic map to guide robotic exploration.\n",
    "\n",
    "### VLMAP\n",
    "Combine visual-language attributes with 3D environmental blueprint. LLM also converts natural language directives into a sequence of navigation objectives that can be pinpointed on the map.\n",
    "\n",
    "\n",
    "### OVRL\n",
    "Neural network architecture consisting of ViTs, convolutions, LSTMs.\n",
    "\n",
    "\n",
    "### ESC\n",
    "Zero-shot object navigation by leveraging commonsense knowledge from pre-trained models. Both grounding, and deduction of relationships between rooms and objects. Probabilistic Soft Locic is used to formulate 'soft' commonsense constraints. These are amalgamated with exploration techniques like Frontier-based exploration.\n",
    "\n",
    "\n",
    "### Nav-GPT\n",
    "\n",
    "\n",
    "\n",
    "### VELMA\n",
    "Identify landmarks from human-authored navigation instructions, CLIP grounds them in current panoramic view for linguistic representation of visual information. The task, trajectory, visual observations are verbalized in natural language, allowing LLM to generate navigate instructions.\n",
    "\n",
    "\n",
    "### $A^2$NAV\n",
    "There are five fundamental action and there are trained navigators for each.\n",
    "GPT-3 predicts action, which is aligned with predefined actions using BERT.\n",
    "\n",
    "\n",
    "### MiC\n",
    "\n",
    "\n",
    "### SayNav\n",
    "\n",
    "\n",
    "### Summary\n",
    "- Grounding using VLM given list of objects in the given environment.\n",
    "- (ESC, Nav-GPT) Commonsense reasoning.\n",
    "- (Nav-GPT) Tracking progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract learning\n",
    "\n",
    "- RL based OSL navigation, where the value function matrix is initially calculated from a matrix of goal-relevance matrix generated by LLM.\n",
    "\n",
    "- Goal-relevance matrix: generated by LLM by reasoning over goal vs given 2D-H language map.\n",
    "\n",
    "- Goal: robotic OSL.\n",
    "\n",
    "- 2D-H language map matrix: initiated as floor. Along the LDS border as index, segmentation output along mid-horizontal line. Fuse LDS reading with segmentation output based on FOV.\n",
    "\n",
    "- Segmentation output: give all objects in the environment and current image to ClipSeg.\n",
    "\n",
    "- LSD border: subscribe to LSD reading, calculate LSD border from LDS reading and current position/orientation.\n",
    "\n",
    "- Current position/orientation: subscribe."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
