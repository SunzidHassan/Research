{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Navigation table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Paper**    | **Paper Link**                   | **Year** | **Design Structure** | **Training Method** | **Dataset**              | **Application**                     | **Source Code**                             | **Inference Time/ Hardware** |\n",
    "|--------------|----------------------------------|----------|----------------------|---------------------|--------------------------|-------------------------------------|---------------------------------------------|------------------------------|\n",
    "| **CoW**      | https://arxiv.org/abs/2203.10421 | 2022     | CLIP                 | Zero-shot           | RoboTHOR                 | Inddor scenes                       | https://github.com/real-stanford/cow.git    |                              |\n",
    "| **ZSON**     | https://arxiv.org/abs/2206.12403 | 2022     | CLIP                 | Fine-tuning         | MP3D                     | Open-world object-goal navigation   | https://github.com/gunagg/zson.git          |                              |\n",
    "| **LM-Nav**   | https://arxiv.org/abs/2207.04429 | 2022     | ViNG, CLIP, GPT-3    | Zero-shot           |                          | Outdoor environments                | https://github.com/blazejosinski/lm_nav.git |                              |\n",
    "| **CLIP-NAV** | https://arxiv.org/abs/2211.16649 | 2022     | CLIP                 | Zero-shot           | REVERIE                  | Household environments              |                                             |                              |\n",
    "| **SQA3D**    | https://arxiv.org/abs/2210.07474 | 2023     | CLIP, BERT, GPT-3    | Fine-tuning         | ScanNet                  | Situated question answering         | https://github.com/SilongYong/SQA3D.git     |                              |\n",
    "| **L3MVN**    | https://arxiv.org/abs/2304.05501 | 2023     | RoBERTa              | Zero-shot           | Gibson, HM3D             | Indoor scenes                       | https://github.com/ybgdgh/L3MVN.git         |                              |\n",
    "| **VLMAP**    | https://arxiv.org/abs/2210.05714 | 2023     | CLIP                 | Zero-shot           | Matterport 3D, Habitat   | Navigation, obstacle map generation | https://github.com/vlmaps/vlmaps.git        |                              |\n",
    "| **OVRL**     | https://arxiv.org/abs/2204.13226 | 2023     | ViT, LSTM            | New architecture         | HM3D, Gibson             | IMAGENAV, OBJECTNAV                 | https://github.com/ykarmesh/OVRL.git        |                              |\n",
    "| **ESC**      | https://arxiv.org/abs/2301.13166 | 2023     | PSL, GLIP            | Zero-shot           | HM3D, RoboTHOR           | Indoor scenes                       |                                             |                              |\n",
    "| **NavGPT**   | https://arxiv.org/abs/2305.16986 | 2023     | GPT-4                | Zero-shot           | R2R, R2R-VLN             | Indoor scenes                       | https://github.com/GengzeZhou/NavGPT.git    |                              |\n",
    "| **VELMA**    | https://arxiv.org/abs/2307.06082 | 2023     | CLIP                 | Fine-tuning         | Touchdown, Map2seq       | Urban VLN                           | https://github.com/raphael-sch/VELMA.git    |                              |\n",
    "| **$A^2$Nav** | https://arxiv.org/abs/2308.07997 | 2023     | GPT-3                | Zero-shot           | R2R-Habitat, RxR-Habitat | Zero-shot navigation                |                                             |                              |\n",
    "| **MiC**      | https://arxiv.org/abs/2308.10141 | 2023     | CLIP                 | Zero-shot           | REVERIE                  | Remote object localization          | https://github.com/YanyuanQiao/MiC.git      |                              |\n",
    "| **SayNav**   | https://arxiv.org/abs/2309.04077 | 2023     | GPT                  |                     | Habitat, Gibson-4+       | Multi-object navigation             |                                             |                              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoW\n",
    "Decomposition of object navigation task into zero-shot object localization and exploration, where CLIP model is used as the object localizer, and there are two exploration methods - learnable and traditional.\n",
    "CoW commences exploration when the object remains undetected and advances towards the target upon its identification. CoW uses three techniques for localization: gradient-based, k-patch-based, k-language-based.\n",
    "\n",
    "\n",
    "### ZSON\n",
    "\n",
    "\n",
    "### LM-Nav (https://arxiv.org/pdf/2207.04429.pdf)\n",
    "- GPT-3 as LLM for decoding verbal instructions into a series of textual landmarks.\n",
    "- CLIP as VLM for anchoring textual landmarks to a topological map.\n",
    "- Vision-Action Model is a self-supervised robotic control model for utilizing visual data and executing physical actions based on plans synthesized by LLM and VLM.\n",
    "\n",
    "\n",
    "### CLIP-Nav\n",
    "Dissect guidance language into critical keyphrases, visually anchor them with CLIP, use the grounding scores and the current state to predict navigation action.\n",
    "\n",
    "\n",
    "### SQA3D\n",
    "\n",
    "\n",
    "### L3MVN\n",
    "2 module: language module handles natural language instructions, generating a semantic map embedded with general physical world knowledge. The later employs this semantic map to guide robotic exploration.\n",
    "\n",
    "### VLMAP\n",
    "Combine visual-language attributes with 3D environmental blueprint. LLM also converts natural language directives into a sequence of navigation objectives that can be pinpointed on the map.\n",
    "\n",
    "\n",
    "### OVRL\n",
    "Neural network architecture consisting of ViTs, convolutions, LSTMs.\n",
    "\n",
    "\n",
    "### ESC\n",
    "Zero-shot object navigation by leveraging commonsense knowledge from pre-trained models. Both grounding, and deduction of relationships between rooms and objects. Probabilistic Soft Locic is used to formulate 'soft' commonsense constraints. These are amalgamated with exploration techniques like Frontier-based exploration.\n",
    "\n",
    "\n",
    "### Nav-GPT\n",
    "\n",
    "\n",
    "\n",
    "### VELMA\n",
    "Identify landmarks from human-authored navigation instructions, CLIP grounds them in current panoramic view for linguistic representation of visual information. The task, trajectory, visual observations are verbalized in natural language, allowing LLM to generate navigate instructions.\n",
    "\n",
    "\n",
    "### $A^2$NAV\n",
    "There are five fundamental action and there are trained navigators for each.\n",
    "GPT-3 predicts action, which is aligned with predefined actions using BERT.\n",
    "\n",
    "\n",
    "### MiC\n",
    "\n",
    "\n",
    "### SayNav\n",
    "\n",
    "\n",
    "### Summary\n",
    "- Grounding using VLM given list of objects in the given environment.\n",
    "- (ESC, Nav-GPT) Commonsense reasoning.\n",
    "- (Nav-GPT) Tracking progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM and VLM for translating goals into reinforcement learning planning for robotic OSL task.\n",
    "\n",
    "- Model based RL for OSL navigation.\n",
    "\n",
    "- Policy iteration for updating state value function using model-based RL approach: run policy iteration (policy evaluation and policy improvement) on LLM-generated model to calculate the vlaue function/policy.\n",
    "\n",
    "- Map the 2D-V semantic segmentation (object relevance scores) to the 2D-H language map: initiated as floor (with value of 0). Take the segmentation value (argmax of object association scores) in the mid-horizontal line of the 2D-V plane. Take the LDS boundary line in the 2D-H plane. Slice the line into 62.2 peices, resulting in segmentation values in each unit angle. Use robot coordinates, orientation, LDS reading from -31.1 to 31.1, LDS angle of reading to get object coordinates of each unit angle. Assign the unit angle segmentation vlaues in those coordinates in the language map.\n",
    "\n",
    "\n",
    "calculate index in language map with LDS reading: given current coordinate and orientation of the robot, and LDS distance and angel, what are the coordinates of the 67 LDS angles (-31.1 to 31.1) in map.\n",
    "\n",
    "align \n",
    "\n",
    "\n",
    "- Segmentation output: give all objects in the environment and current image to ClipSeg. You'll get e 2D-V output of semantic segmentation.\n",
    "\n",
    "- LSD border: check if there is a LDS border in global frame. Otherwise, calculage global LDS border from local frame LSD reading and global position/orientation. These are outlines of objects.\n",
    "\n",
    "- Current position/orientation: subscribe.\n",
    "\n",
    "- Goal-relevance score: for each object in the object list of the environment, assign similarity score to goal with LLM word embedding. Normalize in a way so that every object has negative similarity scores, and only the highly goal-relevant object has positive value.\n",
    "\n",
    "- Exploration using olfaction sensing (approach 1: update value function)\n",
    "\n",
    "- Goal: robotic OSL.\n",
    "\n",
    "\n",
    "Step 1: given the object relevance matrix, run model based RL navigation in turtlebot simulation.\n",
    "Step 2: \n",
    "\n",
    "Issues:\n",
    "- Continuous () vs discrete (map and laser resolution is 0.05m)\n",
    "- Using distance from odor source as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Dyna Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def dyna_q(env, num_episodes, alpha, gamma, epsilon, n_plannings):\n",
    "    # Initialize Q-table\n",
    "    q_table = {}\n",
    "    \n",
    "    # Loop over episodes\n",
    "    for episode in range(num_episodes):\n",
    "        # Initialize state\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Loop until the episode ends\n",
    "        while True:\n",
    "            # Select an action\n",
    "            action = exploration_policy(state, q_table, epsilon)\n",
    "            \n",
    "            # Take a step in the environment\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Update Q-table\n",
    "            update_q(q_table, state, action, reward, next_state, alpha, gamma)\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "            \n",
    "            # Planning step\n",
    "            for _ in range(n_plannings):\n",
    "                # Sample an experience tuple\n",
    "                s, a, r, s_prime = sample_experience(env)\n",
    "                \n",
    "                # Update Q-table\n",
    "                update_q(q_table, s, a, r, s_prime, alpha, gamma)\n",
    "            \n",
    "            # If the episode is done, break\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "    return q_table\n",
    "\n",
    "def update_q(q_table, state, action, reward, next_state, alpha, gamma):\n",
    "    # Get the old Q-value\n",
    "    old_q = q_table.get((state, action), 0.0)\n",
    "    \n",
    "    # Compute the maximum Q-value for the next state\n",
    "    next_max_q = max_q(next_state, q_table)\n",
    "    \n",
    "    # Update the Q-value\n",
    "    q_table[(state, action)] = old_q + alpha * (reward + gamma * next_max_q - old_q)\n",
    "\n",
    "def max_q(state, q_table):\n",
    "    # Initialize the maximum Q-value\n",
    "    max_q_value = -float('inf')\n",
    "    \n",
    "    # Loop over all actions\n",
    "    for action in env.actions:\n",
    "        # Get the Q-value for the current state-action pair\n",
    "        q_value = q_table.get((state, action), 0.0)\n",
    "        \n",
    "        # Update the maximum Q-value\n",
    "        max_q_value = max(max_q_value, q_value)\n",
    "    \n",
    "    return max_q_value\n",
    "\n",
    "# Define the exploration policy (e.g., epsilon-greedy)\n",
    "# and the sample_experience function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
